---
layout: default
title: Microservices auf Commodity Hardware Teil 2 - Continuous Delivery
shortTitle: Microservices und Commodity Hardware Teil 2
documentationExpanded: false
comments: true
postsExpanded: true
categories: microservices spring-boot distributed-systems
published: true
excerpt: excerpt
root: ../../
---

= Scaling out Microservices - Verteilte Systeme auf einfachster Hardware - Teil 2

Schon bevor wir damit beginnen, Nodes und Services aufzusetzen ist es wichtig, dass wir uns eines klar machen -
unser Setup ist nur so gut wie unser Monitoring. Alles steht und fällt mit der Möglichkeit unseren Stack zu monitoren.
Da wir eine _Private Cloud_ aus mehreren Knoten aufbauen werden kümmern wir uns zuerst um simples Monitoring der
einzelnen Knoten.

Wir möchten eine möglichst einheitliche Monitoring-Lösung sowohl für unsere Hardware als auch die darauf deployten
Services.

Zunächst werden wir versuchen, den Ist-Stand zu visualisieren.

Die einfachste Möglichkeit um den Zustand eines Knoten in unserem Cluster zu visualisieren ist der Login per SSH und
ausführen von _top_ oder _htop_.

image::/assets/images/htop.png[htop,800]

Problem hierbei - wir sehen Probleme nur, wenn wir zufälligerweise zur richtigen Zeit auf dem richtigen Knoten
eingeloggt sind und dort _htop_ ausführen. Das ist nicht praktikabel - _und mir ehrlich gesagt zu anstrengend_...

Stattdessen wäre es praktisch, einen Ort zu haben an dem alle Informationen zu allen Knoten aggregiert zur Verfügung
stehen. Ein Dashboard, das uns auf einen Blick alle wichtigen Informationen bereitstellt.

== Monitoring

Es gibt eine ganze Menge an frei verfügbaren und auch kostenpflichtigen Tools, um ein derartiges Dashboard möglich zu
 machen. Ich habe Erfahrung mit einigen dieser Tools und mit dem meiner Ansicht nach besten arbeiten wir für diese
 Projekt.

Zunächst benötigen wir eine Möglichkeit, die Informationen, die wir von allen Knoten einsammeln zu speichern. Denn
nicht nur die Momentaufnahme ist wichtig sondern auch der zeitliche Verlauf. Ein großes Problem sind beispielsweise
Memory-Leaks. Services laufen oft eine ganze Zeit ohne Probleme bis sie nach einigen Tagen oder Wochen plötzlich
langsam werden. Die typische Reaktion darauf ist die Services in regelmäßigen Abständen durchzustarten.
Hat man keinen zeitlichen Verlauf und kann nicht nachvollziehen, wie sich beispielsweise der Speicherverbrauch seit
dem letzten Restart entwickelt hat ist es äußerst schwer die echte Ursache zu finden.

== InfluxDB

Eines der besten frei verfügbaren Tools ist https://influxdata.com/[InfluxDB], eine Time-Series-Datenbank aktuell in
der Version 0.13 und geschrieben in Go und daher problemlauf lauffähig auf einem Raspberry-PI - Ideal für dieses
Projekt.

Was ist eine *Time-Series-DB*?
----

In einer TimeSeriesDB speichert man vereinfacht ein Tupel "Zeitpunkt / Wert" und einer Menge an Tags.
Um 10:00 wurden 9,87GB Speicher verbraucht. Tags - host=192.168.178.25
Um 10:05 wurden 12,76GB Speicher verbraucht. Tags - host=192.168.178.25
----

Aus diesen Daten lassen sich Graphen und Diagramme erstellen (hier ein Beispiel aus JConsole).

image::/assets/images/jconsole.png[Heap Allocation in JConsole, 800]

Natürlich lassen sich derartige Informationen auch in einer herkömmlichen Datenbank speichern. Eine Time-Series-DB
ist allerdings spezialisiert, große Datenmengen zu speichern und effizient zu verwalten. Das können bis zu Millionen
von Tupeln pro Sekunde sein.

Auch die InfluxDB kann problemlos in einem Container betrieben werden.

TIP: Loggen Sie sich auf einem der Knoten ein.

[source, bash]
----
docker run -d -p 8083:8083 -p 8086:8086 -p 25826:25826/udp dilgerm/rpi-influxdb:0.13
----

Die InfluxDB arbeitt standardmäßig auf den Ports *8083*, *8086* und *25826*. Über den Port 8086 bietet die InfluxDB
eine HTTP API für Schreib / Lesezugriffe. Über den Port 8083 bietet die InfluxDB eine einfache Web-UI.

Nachdem wir die Datenbank bereits gestartet wir die notwendigen Ports bereits exposed haben können wir direkt auf die
 Datenbank zugreifen.

TIP: In meinem Beispiel läuft die Influx auf dem Host mit der IP 192.168.178.25

[source, bash]
----
http://192.168.178.25:8086
----

Öffnen wir diese Seite im Browser zeigt sich die einfache und funktionale Influx-UI.

image::/assets/images/influx-ui.png[Influx UI, 800]

Die InfluxDB bietet eine SQL-ähnliche Abfragesprache - InfluxQL um auf die Daten in der Datenbank zuzugreifen. Die UI
bietet uns ein Eingabefeld, über das wir direkt Abfragen gegen die Datenbank absetzen können. Alternativ geht auch
deie Abfrage über den HTTP-Endpoint.
Standardmäßig schreibt die Influx ihre eigenen Metriken in eine Datenbank mit dem Namen __internal_.

Wir können den Inhalt einer Datenbank sehr einfach über dne Http-Endpoint abrufen.

[source, bash]
----
curl http://192.168.178.25:8086/query?q=SHOW+MEASUREMENTS&db=_internal
----

Die Query _SHOW MEASUREMENTS_ liefert für die Datenbank __internal_ folgende Werte.

* httpd
* runtime
* shard
* subscriber
* write

In der Serie _httpd_ beispielsweise speichert die InfluxDB Meta-Daten über die HTTP Zugriffe auf die Datenbank.

[source, bash]
----
select * from httpd
----

image::/assets/images/influx_httpd.png[Influx HTTPD, 800]

== Telegraf

Influx bietet eine Komplettlösung zum Thema Monitoring, den sogenannten TICK-Stack (Telegraf, Influx, Chronograph,
Kapacitor). Das I in TICK haben wir bereits, wir arbeiten uns jetzt weiter zu *T* - Telegraf. Telegraf ist eine
Daemon ganz ähnlich beispielsweise *CollectD*, der pro System installiert wird und der Metriken über das System an die
Influx weiterleitet.

TIP: Das aktuellen Vorgehen besteht darin, einen _Proof of Concept_ für unsere Monitoringlösung zu erarbeiten, das
heisst, wir möchten so schnell wie möglich eine lauffähige Lösung die funktioniert. Im zweiten Schritt kümmern wir
uns dann darum, das ganze auch wartbar zu bekommen.

DIe Installation für Telefgraf ist ähnlich einfach wie für die Influx, da bereits fertige Binaries für ARM angeboten
werden. Der Link auf die Last-Stable Version von Telegraf findet sich auf der https://github.com/influxdata/telegraf[Github-Seite].

[source, bash]
----
wget https://dl.influxdata.com/telegraf/releases/telegraf_1.0.0-beta1_armhf.deb
sudo dpkg -i telegraf_1.0.0-beta1_armhf.deb
sudo systemctl start telegraf
----

Das wars, damit haben wir eine Telegraf-Instanz mit _Default_-Konfiguration gestartet.

Es lässt sich zunächst sehr einfach überprüfen, ob die Telegraf-Instanz läuft.

[source, bash]
----
sudo systemctl status telegraf
● telegraf.service - The plugin-driven server agent for reporting metrics into InfluxDB
   Loaded: loaded (/lib/systemd/system/telegraf.service; enabled)
   Active: active (running) since Fri 2016-06-10 12:48:45 UTC; 17min ago
     Docs: https://github.com/influxdata/telegraf
 Main PID: 927 (telegraf)
   CGroup: /system.slice/telegraf.service
           └─927 /usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d

Jun 10 12:48:45 pi25 systemd[1]: Started The plugin-driven server agent for reporting metrics into InfluxDB.
Jun 10 12:49:06 pi25 systemd[1]: Started The plugin-driven server agent for reporting metrics into InfluxDB.
----

Die Konfiguration wird standardmäßig in _/etc/telegraf/telegraf.conf_.

In der Outputs-Konfiguration findet sich die Plugin-Sektion.

[source, bash]
----
[[outputs.influxdb]]
 urls = ["http://localhost:8086"]
----

Weitere Konfiguration ist zunächst nicht notwendig. Werfen wir einen weiteren Blick auf die Influx-UI sollten wir
hier bereits die neu angelegte Datenbank _telegraf_ sehen.

image::/assets/images/telegraf.png[Telegraf UI, 800]

Betrachten wir die standardmäßig getrackten Daten finden sich folgende Serien.

[source, bash]
----
cpu
disk
diskio
kernel
mem
processes
swap
system
----

Wir werden diese Liste noch erweitern. Für den Anfang ist das aber genug, da wir problemlos die wichtigsten
Systemmetriken ablesen können.

=== Grafana

Wir haben jetzt die Datenquelle (Telegraf) und die Datenhaltung (InfluxDB). Was fehlt ist die Visualisierung. Wir
werden hierfür nicht das *C* in TI*C*K verwenden sondern Grafana.

Grafana selbst bietet keine Binaries für ARM, es gibt aber bereits einige https://hub.docker.com/r/fg2it/grafana-armhf/[Repositories], die passende Docker-Images
bereitstellen.

[source, bash]
----
docker run -d -i -p 3000:3000 --name grafana fg2it/grafana-armhf:v3.0.4
----

Nachdem das Docker Image geladen wurde können wir auf die Oberfläche unter http://<ip des raspberries>:3000 zugreifen.
Die IP müssen Sie natürlich entsprechend anpassen.

Wir betreiben in diesem Moment den Telegraf-Prozess, InfluxDB und Grafana auf einem einzigen Raspberry PI.
Eine hoch interessante Frage ist, inwiefern lasten diese Tools den Raspberry bereits aus.
Versuchen wir diese Fragen zu beantworten.

Zunächst einmal müssen wir eine Grafana-Datasource anlegen für die InfluxDB.

image::/assets/images/influx-datasource.png[Influx Telegraf Datadource, 800]

Konfigurieren wir uns jetzt ein neues Dashboard und einen Graphen für die CPU Auslastung sehen wir, dass wir aktuell
trotz einem eher schwergewichtigen Prozess wir der InfluxDB quasi keine CPU Auslastung im System haben.

So lassen sich sehr schnell die wichtigsten Metriken visualisieren.

image::/assets/images/system-metrics.png[System Metriken, 800]

Das hier visualisierte Dashboard ist http://{{site.url}}/assets/dashboards/systems-dashboard-simple-1.json[hier]
testweise hinterlegt und kann direkt in Grafana importiert werden.

TODO Videolink

Es stellt sich die Frage, wieviel speichert Influx hier eigentlich an Daten. Wenn nichts weiter konfiguriert
wird werden die Daten von Telegraf alle 10 Sekunden nach Influx geflushed.
Influx selbst speichert zunächst alle Daten in einem WAL-Log. Das ist eine Append-Only / Read-Only Datenstruktur, wie
 sie eigentlich von jeder gängigen Datenbank heutzutage verwendet wird.
Sobald die Größe des WAL-Log einen bestimmten Schwellwert übersteigt werden die Daten in einer separate Datenstruktur
 abgelegt (TSM-Tree Time Structured Merge Tree).

TIP: Nach 6 Stunden _Normalbetrieb_ hat die Influx im WAL-Log für einen Knoten ungefähr 6 Megabyte von Telegraf
gespeichert. Für einen Knoten wären das also im Rohformat ungefähr 700 Megabyte. Das lässt sich noch stark
optimieren, doch darüber reden wir später.

TODO /influxdb/usr/bin# ./influx_inspect
TODO /influxdb/usr/bin# ./influx_stress

Solange nur ein Knoten überwacht wird ist das aber wenig spannend, viel spannender wird es, wenn wir den gesamten
Cluster überwachen. Die Annahme wäre jetz, dass wir, sobald Telegraf auf einem zweiten Knoten mit der korrekten
Konfiguration installiert wird dies automatisch in Grafana sehen werden.
Testweise werden wir nochmal manuell einen zweiten Knoten mit Telegraf bespielen.

TIP: Ich verwende hierfür einen zweiten Raspberry im Cluster mit der IP 192.168.178.24

Keine Angst, das ist einer der letzten manuellen Schritte die wir hierfür ausführen werden.

Hierfür loggen wir uns per SSH auf einem zweiten Knoten ein und installieren erneut Telegraf.

[source, bash]
----
wget https://dl.influxdata.com/telegraf/releases/telegraf_1.0.0-beta1_armhf.deb
sudo dpkg -i telegraf_1.0.0-beta1_armhf.deb
sudo systemctl start telegraf
----

Anschließend editieren wir den Telegraf-Konfiguration und hinterlegen dort die korrekte URI auf die InfluxDB. Denn
auf diesem Host greift der Default mit _localhost_ natürlich nicht.

[source, bash]
----
sudo vi /etc/telegraf/telegraf.conf

#urls = ["http://localhost:8086"] # required ersetzen mit
urls = ["http://192.168.178.25:8086"] # required

#anchließend restart
sudo systemctl restart telegraf
----

TODO Video

Ohne weitere Konfiguration in Grafana erscheint sofort der neue Host Pi24 (jetzt in grün), der erwartungsgemäß im
Vergleich zum Pi25 absolut gelangweilt ist einfach nichts zu tun hat. Das wird sich bald ändern.

image::/assets/images/2_telegraf_hosts.png[Zwei Hosts, 800]

TODO :/influxdb/usr/bin/influx_inspect
TODO :/influxdb/usr/bin/influx_stress

=== Ansible

Natürlich könnten wir jetzt Host für Host einzeln mit Telegraf bestücken um ordentliches Monitoring zu bekommen.
So viel Zeit haben wir aber nicht.

== Notifications
